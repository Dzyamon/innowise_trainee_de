{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "trained-moses",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "induced-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import max, avg, min, col, row_number, when, desc, monotonically_increasing_id, lit, rank, expr\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-photographer",
   "metadata": {},
   "source": [
    "create local SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stock-partnership",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 20:33:03 WARN Utils: Your hostname, user-thinkpad-e15-gen-2 resolves to a loopback address: 127.0.1.1; using 10.80.51.3 instead (on interface wlp0s20f3)\n",
      "24/02/06 20:33:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/06 20:33:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.1 ms, sys: 30.3 ms, total: 71.4 ms\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('Spark_App') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# local[*] - use all logical cpu cores = number of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-blame",
   "metadata": {},
   "source": [
    "read csv with inferschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "computational-liverpool",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.56 ms, sys: 382 Âµs, total: 4.94 ms\n",
      "Wall time: 6.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read \\\n",
    "    .options(header='True', inferSchema='True', delimiter=',') \\\n",
    "    .format(\"csv\") \\\n",
    "    .load(\"ds_salaries.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-dominant",
   "metadata": {},
   "source": [
    "read csv one more time with the same code and you will see that it almostly don\"t take time, because info already in SparkSession and it will not read nothing\n",
    "from this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aging-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.38 ms, sys: 1.59 ms, total: 6.96 ms\n",
      "Wall time: 328 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .csv(\"ds_salaries.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-tomorrow",
   "metadata": {},
   "source": [
    "write schema of csv on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "least-communications",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- work_year: integer (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: integer (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: integer (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-brother",
   "metadata": {},
   "source": [
    "create schema of this csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "progressive-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), nullable=False),\n",
    "    StructField(\"work_year\", IntegerType(), nullable=False),\n",
    "    StructField(\"experience_level\", StringType(), nullable=False),\n",
    "    StructField(\"employment_type\", StringType(), nullable=False),\n",
    "    StructField(\"job_title\", StringType(), nullable=False),\n",
    "    StructField(\"salary\", IntegerType(), nullable=False),\n",
    "    StructField(\"salary_currency\", StringType(), nullable=False),\n",
    "    StructField(\"salary_in_usd\", DoubleType(), nullable=False),\n",
    "    StructField(\"employee_residence\", StringType(), nullable=False),\n",
    "    StructField(\"remote_ratio\", IntegerType(), nullable=False),\n",
    "    StructField(\"company_location\", StringType(), nullable=False),\n",
    "    StructField(\"company_size\", StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-sauce",
   "metadata": {},
   "source": [
    "restart kernel without cleaning output and after restarting you need to initialize SparkSession, after initialize start execute only cells from cell with schema=\n",
    "=StructType.... \n",
    "To restart kernel click Kernel, Restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import max, avg, min, col, row_number, when, desc, monotonically_increasing_id, lit, rank, expr\n",
    "from pyspark.sql.window import Window"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 20:35:52 WARN Utils: Your hostname, user-thinkpad-e15-gen-2 resolves to a loopback address: 127.0.1.1; using 10.80.51.3 instead (on interface wlp0s20f3)\n",
      "24/02/06 20:35:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/06 20:36:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('Spark_App') \\\n",
    "                    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), nullable=False),\n",
    "    StructField(\"work_year\", IntegerType(), nullable=False),\n",
    "    StructField(\"experience_level\", StringType(), nullable=False),\n",
    "    StructField(\"employment_type\", StringType(), nullable=False),\n",
    "    StructField(\"job_title\", StringType(), nullable=False),\n",
    "    StructField(\"salary\", IntegerType(), nullable=False),\n",
    "    StructField(\"salary_currency\", StringType(), nullable=False),\n",
    "    StructField(\"salary_in_usd\", DoubleType(), nullable=False),\n",
    "    StructField(\"employee_residence\", StringType(), nullable=False),\n",
    "    StructField(\"remote_ratio\", IntegerType(), nullable=False),\n",
    "    StructField(\"company_location\", StringType(), nullable=False),\n",
    "    StructField(\"company_size\", StringType(), nullable=False)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "artistic-hospital",
   "metadata": {},
   "source": [
    "read ds_salaries with predefined schema and compare results from this cell and cell with inferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "literary-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(\"ds_salaries.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-joint",
   "metadata": {},
   "source": [
    "this happens because read operation is lazy(transformation), but if you use inferschema it start to be action that will create Spark Job, because Spark need to loop throw all file to check datatypes for all columns and this can harm to your code(if we compare to parquet, it will also go to check data types, but parquet provide meta information, so Spark will not go throw all file, he will just read meta information, but csv don\"t provide such meta information). Also header make Spark to create one more Spark Job to check first line to define name of columns and remember to skip it when reading. Actual reading start when you will use first action. More about Spark Jobs you will see in next topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-assurance",
   "metadata": {},
   "source": [
    "write schema of scv on screen one more time and compare with previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "solid-infection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- work_year: integer (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: double (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: integer (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-water",
   "metadata": {},
   "source": [
    "now continue to work with one of the dataframes that you create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-belgium",
   "metadata": {},
   "source": [
    "print data in dataframe using df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legendary-alarm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+---------------+--------------------+--------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "| id|work_year|experience_level|employment_type|           job_title|  salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|\n",
      "+---+---------+----------------+---------------+--------------------+--------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|  0|     2020|              MI|             FT|      Data Scientist|   70000|            EUR|      79833.0|                DE|           0|              DE|           L|\n",
      "|  1|     2020|              SE|             FT|Machine Learning ...|  260000|            USD|     260000.0|                JP|           0|              JP|           S|\n",
      "|  2|     2020|              SE|             FT|   Big Data Engineer|   85000|            GBP|     109024.0|                GB|          50|              GB|           M|\n",
      "|  3|     2020|              MI|             FT|Product Data Analyst|   20000|            USD|      20000.0|                HN|           0|              HN|           S|\n",
      "|  4|     2020|              SE|             FT|Machine Learning ...|  150000|            USD|     150000.0|                US|          50|              US|           L|\n",
      "|  5|     2020|              EN|             FT|        Data Analyst|   72000|            USD|      72000.0|                US|         100|              US|           L|\n",
      "|  6|     2020|              SE|             FT| Lead Data Scientist|  190000|            USD|     190000.0|                US|         100|              US|           S|\n",
      "|  7|     2020|              MI|             FT|      Data Scientist|11000000|            HUF|      35735.0|                HU|          50|              HU|           L|\n",
      "|  8|     2020|              MI|             FT|Business Data Ana...|  135000|            USD|     135000.0|                US|         100|              US|           L|\n",
      "|  9|     2020|              SE|             FT|  Lead Data Engineer|  125000|            USD|     125000.0|                NZ|          50|              NZ|           S|\n",
      "| 10|     2020|              EN|             FT|      Data Scientist|   45000|            EUR|      51321.0|                FR|           0|              FR|           S|\n",
      "| 11|     2020|              MI|             FT|      Data Scientist| 3000000|            INR|      40481.0|                IN|           0|              IN|           L|\n",
      "| 12|     2020|              EN|             FT|      Data Scientist|   35000|            EUR|      39916.0|                FR|           0|              FR|           M|\n",
      "| 13|     2020|              MI|             FT|   Lead Data Analyst|   87000|            USD|      87000.0|                US|         100|              US|           L|\n",
      "| 14|     2020|              MI|             FT|        Data Analyst|   85000|            USD|      85000.0|                US|         100|              US|           L|\n",
      "| 15|     2020|              MI|             FT|        Data Analyst|    8000|            USD|       8000.0|                PK|          50|              PK|           L|\n",
      "| 16|     2020|              EN|             FT|       Data Engineer| 4450000|            JPY|      41689.0|                JP|         100|              JP|           S|\n",
      "| 17|     2020|              SE|             FT|   Big Data Engineer|  100000|            EUR|     114047.0|                PL|         100|              GB|           S|\n",
      "| 18|     2020|              EN|             FT|Data Science Cons...|  423000|            INR|       5707.0|                IN|          50|              IN|           M|\n",
      "| 19|     2020|              MI|             FT|  Lead Data Engineer|   56000|            USD|      56000.0|                PT|         100|              US|           M|\n",
      "+---+---------+----------------+---------------+--------------------+--------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-medium",
   "metadata": {},
   "source": [
    "print data in dataframe using display(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "connected-dryer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      id  work_year experience_level employment_type  \\\n0      0       2020               MI              FT   \n1      1       2020               SE              FT   \n2      2       2020               SE              FT   \n3      3       2020               MI              FT   \n4      4       2020               SE              FT   \n..   ...        ...              ...             ...   \n602  602       2022               SE              FT   \n603  603       2022               SE              FT   \n604  604       2022               SE              FT   \n605  605       2022               SE              FT   \n606  606       2022               MI              FT   \n\n                      job_title  salary salary_currency  salary_in_usd  \\\n0                Data Scientist   70000             EUR        79833.0   \n1    Machine Learning Scientist  260000             USD       260000.0   \n2             Big Data Engineer   85000             GBP       109024.0   \n3          Product Data Analyst   20000             USD        20000.0   \n4     Machine Learning Engineer  150000             USD       150000.0   \n..                          ...     ...             ...            ...   \n602               Data Engineer  154000             USD       154000.0   \n603               Data Engineer  126000             USD       126000.0   \n604                Data Analyst  129000             USD       129000.0   \n605                Data Analyst  150000             USD       150000.0   \n606                AI Scientist  200000             USD       200000.0   \n\n    employee_residence  remote_ratio company_location company_size  \n0                   DE             0               DE            L  \n1                   JP             0               JP            S  \n2                   GB            50               GB            M  \n3                   HN             0               HN            S  \n4                   US            50               US            L  \n..                 ...           ...              ...          ...  \n602                 US           100               US            M  \n603                 US           100               US            M  \n604                 US             0               US            M  \n605                 US           100               US            M  \n606                 IN           100               US            L  \n\n[607 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>work_year</th>\n      <th>experience_level</th>\n      <th>employment_type</th>\n      <th>job_title</th>\n      <th>salary</th>\n      <th>salary_currency</th>\n      <th>salary_in_usd</th>\n      <th>employee_residence</th>\n      <th>remote_ratio</th>\n      <th>company_location</th>\n      <th>company_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2020</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Data Scientist</td>\n      <td>70000</td>\n      <td>EUR</td>\n      <td>79833.0</td>\n      <td>DE</td>\n      <td>0</td>\n      <td>DE</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Machine Learning Scientist</td>\n      <td>260000</td>\n      <td>USD</td>\n      <td>260000.0</td>\n      <td>JP</td>\n      <td>0</td>\n      <td>JP</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Big Data Engineer</td>\n      <td>85000</td>\n      <td>GBP</td>\n      <td>109024.0</td>\n      <td>GB</td>\n      <td>50</td>\n      <td>GB</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2020</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Product Data Analyst</td>\n      <td>20000</td>\n      <td>USD</td>\n      <td>20000.0</td>\n      <td>HN</td>\n      <td>0</td>\n      <td>HN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Machine Learning Engineer</td>\n      <td>150000</td>\n      <td>USD</td>\n      <td>150000.0</td>\n      <td>US</td>\n      <td>50</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>602</th>\n      <td>602</td>\n      <td>2022</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Data Engineer</td>\n      <td>154000</td>\n      <td>USD</td>\n      <td>154000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>603</th>\n      <td>603</td>\n      <td>2022</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Data Engineer</td>\n      <td>126000</td>\n      <td>USD</td>\n      <td>126000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>604</th>\n      <td>604</td>\n      <td>2022</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Data Analyst</td>\n      <td>129000</td>\n      <td>USD</td>\n      <td>129000.0</td>\n      <td>US</td>\n      <td>0</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>605</th>\n      <td>605</td>\n      <td>2022</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Data Analyst</td>\n      <td>150000</td>\n      <td>USD</td>\n      <td>150000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>606</th>\n      <td>606</td>\n      <td>2022</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>AI Scientist</td>\n      <td>200000</td>\n      <td>USD</td>\n      <td>200000.0</td>\n      <td>IN</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n  </tbody>\n</table>\n<p>607 rows Ã 12 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df2.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "create df_job_title that consists from all job_titles without duplicates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "friendly-cartridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           job_title|\n",
      "+--------------------+\n",
      "|3D Computer Visio...|\n",
      "|  Lead Data Engineer|\n",
      "|Head of Machine L...|\n",
      "|     Data Specialist|\n",
      "| Data Analytics Lead|\n",
      "|Machine Learning ...|\n",
      "|   Lead Data Analyst|\n",
      "|Data Engineering ...|\n",
      "|Staff Data Scientist|\n",
      "|       ETL Developer|\n",
      "|Director of Data ...|\n",
      "|Product Data Analyst|\n",
      "|Principal Data Sc...|\n",
      "|        AI Scientist|\n",
      "|Director of Data ...|\n",
      "|Machine Learning ...|\n",
      "| Lead Data Scientist|\n",
      "|Machine Learning ...|\n",
      "|Data Science Engi...|\n",
      "|Machine Learning ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Distinct count: 50\n"
     ]
    }
   ],
   "source": [
    "df_job_title = df2.select(\"job_title\").distinct()\n",
    "df_job_title.show()\n",
    "print(\"Distinct count:\", df_job_title.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "     id  work_year experience_level employment_type  \\\n0    77       2021               MI              PT   \n1    52       2020               EN              FT   \n2   344       2022               EX              FT   \n3    82       2021               MI              FT   \n4   132       2021               MI              FT   \n5    23       2020               MI              FT   \n6   255       2021               SE              FT   \n7     2       2020               SE              FT   \n8     8       2020               MI              FT   \n9    95       2021               MI              FT   \n10   54       2020               SE              FL   \n11   98       2021               EN              FT   \n12    5       2020               EN              FT   \n13   80       2021               SE              FT   \n14  523       2022               SE              FT   \n15  158       2021               SE              FT   \n16  169       2021               MI              FT   \n17   16       2020               EN              FT   \n18   30       2020               MI              FT   \n19   18       2020               EN              FT   \n20  117       2021               MI              FT   \n21   67       2020               SE              FT   \n22    0       2020               MI              FT   \n23  165       2021               SE              FT   \n24  111       2021               SE              FT   \n25   25       2020               EX              FT   \n26  373       2022               MI              FT   \n27  183       2021               SE              FT   \n28   97       2021               MI              FT   \n29   74       2021               EX              FT   \n30  161       2021               EX              FT   \n31  384       2022               EX              FT   \n32   13       2020               MI              FT   \n33    9       2020               SE              FT   \n34    6       2020               SE              FT   \n35  457       2022               SE              FT   \n36   45       2020               EN              PT   \n37  130       2021               EN              FT   \n38    4       2020               SE              FT   \n39   42       2020               MI              FT   \n40   29       2020               SE              FT   \n41    1       2020               SE              FT   \n42   90       2021               SE              FT   \n43  455       2022               MI              FT   \n44  249       2021               SE              FT   \n45  121       2021               SE              FT   \n46   55       2020               SE              FT   \n47    3       2020               MI              FT   \n48   26       2020               EN              FT   \n49  283       2021               SE              CT   \n\n                                   job_title   salary salary_currency  \\\n0              3D Computer Vision Researcher   400000             INR   \n1                               AI Scientist   300000             DKK   \n2                         Analytics Engineer   175000             USD   \n3                     Applied Data Scientist    68000             CAD   \n4         Applied Machine Learning Scientist    38400             USD   \n5                            BI Data Analyst    98000             USD   \n6                         Big Data Architect   125000             CAD   \n7                          Big Data Engineer    85000             GBP   \n8                      Business Data Analyst   135000             USD   \n9                        Cloud Data Engineer   120000             SGD   \n10                  Computer Vision Engineer    60000             USD   \n11         Computer Vision Software Engineer    70000             USD   \n12                              Data Analyst    72000             USD   \n13                   Data Analytics Engineer    67000             EUR   \n14                       Data Analytics Lead   405000             USD   \n15                    Data Analytics Manager   120000             USD   \n16                            Data Architect   150000             USD   \n17                             Data Engineer  4450000             JPY   \n18                  Data Engineering Manager    51999             EUR   \n19                   Data Science Consultant   423000             INR   \n20                     Data Science Engineer    34000             EUR   \n21                      Data Science Manager   190200             USD   \n22                            Data Scientist    70000             EUR   \n23                           Data Specialist   165000             USD   \n24              Director of Data Engineering    82500             GBP   \n25                  Director of Data Science   325000             USD   \n26                             ETL Developer    50000             EUR   \n27                      Finance Data Analyst    45000             GBP   \n28                    Financial Data Analyst   450000             USD   \n29                              Head of Data   235000             USD   \n30                      Head of Data Science    85000             USD   \n31                  Head of Machine Learning  6000000             INR   \n32                         Lead Data Analyst    87000             USD   \n33                        Lead Data Engineer   125000             USD   \n34                       Lead Data Scientist   190000             USD   \n35            Lead Machine Learning Engineer    80000             EUR   \n36                               ML Engineer    14000             EUR   \n37                Machine Learning Developer   100000             USD   \n38                 Machine Learning Engineer   150000             USD   \n39  Machine Learning Infrastructure Engineer    44000             EUR   \n40                  Machine Learning Manager   157000             CAD   \n41                Machine Learning Scientist   260000             USD   \n42                    Marketing Data Analyst    75000             EUR   \n43                              NLP Engineer   240000             CNY   \n44                    Principal Data Analyst   170000             USD   \n45                   Principal Data Engineer   200000             USD   \n46                  Principal Data Scientist   130000             EUR   \n47                      Product Data Analyst    20000             USD   \n48                        Research Scientist    42000             USD   \n49                      Staff Data Scientist   105000             USD   \n\n    salary_in_usd employee_residence  remote_ratio company_location  \\\n0          5409.0                 IN            50               IN   \n1         45896.0                 DK            50               DK   \n2        175000.0                 US           100               US   \n3         54238.0                 GB            50               CA   \n4         38400.0                 VN           100               US   \n5         98000.0                 US             0               US   \n6         99703.0                 CA            50               CA   \n7        109024.0                 GB            50               GB   \n8        135000.0                 US           100               US   \n9         89294.0                 SG            50               SG   \n10        60000.0                 RU           100               US   \n11        70000.0                 US           100               US   \n12        72000.0                 US           100               US   \n13        79197.0                 DE           100               DE   \n14       405000.0                 US           100               US   \n15       120000.0                 US           100               US   \n16       150000.0                 US           100               US   \n17        41689.0                 JP           100               JP   \n18        59303.0                 DE           100               DE   \n19         5707.0                 IN            50               IN   \n20        40189.0                 GR           100               GR   \n21       190200.0                 US           100               US   \n22        79833.0                 DE             0               DE   \n23       165000.0                 US           100               US   \n24       113476.0                 GB           100               GB   \n25       325000.0                 US           100               US   \n26        54957.0                 GR             0               GR   \n27        61896.0                 GB            50               GB   \n28       450000.0                 US           100               US   \n29       235000.0                 US           100               US   \n30        85000.0                 RU             0               RU   \n31        79039.0                 IN            50               IN   \n32        87000.0                 US           100               US   \n33       125000.0                 NZ            50               NZ   \n34       190000.0                 US           100               US   \n35        87932.0                 DE             0               DE   \n36        15966.0                 DE           100               DE   \n37       100000.0                 IQ            50               IQ   \n38       150000.0                 US            50               US   \n39        50180.0                 PT             0               PT   \n40       117104.0                 CA            50               CA   \n41       260000.0                 JP             0               JP   \n42        88654.0                 GR           100               DK   \n43        37236.0                 US            50               US   \n44       170000.0                 US           100               US   \n45       200000.0                 US           100               US   \n46       148261.0                 DE           100               DE   \n47        20000.0                 HN             0               HN   \n48        42000.0                 NL            50               NL   \n49       105000.0                 US           100               US   \n\n   company_size  \n0             M  \n1             S  \n2             M  \n3             L  \n4             M  \n5             M  \n6             M  \n7             M  \n8             L  \n9             L  \n10            S  \n11            M  \n12            L  \n13            L  \n14            L  \n15            M  \n16            L  \n17            S  \n18            S  \n19            M  \n20            M  \n21            M  \n22            L  \n23            L  \n24            M  \n25            L  \n26            M  \n27            L  \n28            L  \n29            L  \n30            M  \n31            L  \n32            L  \n33            S  \n34            S  \n35            M  \n36            S  \n37            S  \n38            L  \n39            M  \n40            L  \n41            S  \n42            L  \n43            L  \n44            M  \n45            M  \n46            M  \n47            S  \n48            L  \n49            M  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>work_year</th>\n      <th>experience_level</th>\n      <th>employment_type</th>\n      <th>job_title</th>\n      <th>salary</th>\n      <th>salary_currency</th>\n      <th>salary_in_usd</th>\n      <th>employee_residence</th>\n      <th>remote_ratio</th>\n      <th>company_location</th>\n      <th>company_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>77</td>\n      <td>2021</td>\n      <td>MI</td>\n      <td>PT</td>\n      <td>3D Computer Vision Researcher</td>\n      <td>400000</td>\n      <td>INR</td>\n      <td>5409.0</td>\n      <td>IN</td>\n      <td>50</td>\n      <td>IN</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>52</td>\n      <td>2020</td>\n      <td>EN</td>\n      <td>FT</td>\n      <td>AI Scientist</td>\n      <td>300000</td>\n      <td>DKK</td>\n      <td>45896.0</td>\n      <td>DK</td>\n      <td>50</td>\n      <td>DK</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>344</td>\n      <td>2022</td>\n      <td>EX</td>\n      <td>FT</td>\n      <td>Analytics Engineer</td>\n      <td>175000</td>\n      <td>USD</td>\n      <td>175000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>82</td>\n      <td>2021</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Applied Data Scientist</td>\n      <td>68000</td>\n      <td>CAD</td>\n      <td>54238.0</td>\n      <td>GB</td>\n      <td>50</td>\n      <td>CA</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>132</td>\n      <td>2021</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Applied Machine Learning Scientist</td>\n      <td>38400</td>\n      <td>USD</td>\n      <td>38400.0</td>\n      <td>VN</td>\n      <td>100</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>23</td>\n      <td>2020</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>BI Data Analyst</td>\n      <td>98000</td>\n      <td>USD</td>\n      <td>98000.0</td>\n      <td>US</td>\n      <td>0</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>255</td>\n      <td>2021</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Big Data Architect</td>\n      <td>125000</td>\n      <td>CAD</td>\n      <td>99703.0</td>\n      <td>CA</td>\n      <td>50</td>\n      <td>CA</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Big Data Engineer</td>\n      <td>85000</td>\n      <td>GBP</td>\n      <td>109024.0</td>\n      <td>GB</td>\n      <td>50</td>\n      <td>GB</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>2020</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Business Data Analyst</td>\n      <td>135000</td>\n      <td>USD</td>\n      <td>135000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>95</td>\n      <td>2021</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Cloud Data Engineer</td>\n      <td>120000</td>\n      <td>SGD</td>\n      <td>89294.0</td>\n      <td>SG</td>\n      <td>50</td>\n      <td>SG</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>54</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FL</td>\n      <td>Computer Vision Engineer</td>\n      <td>60000</td>\n      <td>USD</td>\n      <td>60000.0</td>\n      <td>RU</td>\n      <td>100</td>\n      <td>US</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>98</td>\n      <td>2021</td>\n      <td>EN</td>\n      <td>FT</td>\n      <td>Computer Vision Software Engineer</td>\n      <td>70000</td>\n      <td>USD</td>\n      <td>70000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5</td>\n      <td>2020</td>\n      <td>EN</td>\n      <td>FT</td>\n      <td>Data Analyst</td>\n      <td>72000</td>\n      <td>USD</td>\n      <td>72000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>80</td>\n      <td>2021</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Data Analytics Engineer</td>\n      <td>67000</td>\n      <td>EUR</td>\n      <td>79197.0</td>\n      <td>DE</td>\n      <td>100</td>\n      <td>DE</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>523</td>\n      <td>2022</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Data Analytics Lead</td>\n      <td>405000</td>\n      <td>USD</td>\n      <td>405000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>158</td>\n      <td>2021</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Data Analytics Manager</td>\n      <td>120000</td>\n      <td>USD</td>\n      <td>120000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>169</td>\n      <td>2021</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Data Architect</td>\n      <td>150000</td>\n      <td>USD</td>\n      <td>150000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>16</td>\n      <td>2020</td>\n      <td>EN</td>\n      <td>FT</td>\n      <td>Data Engineer</td>\n      <td>4450000</td>\n      <td>JPY</td>\n      <td>41689.0</td>\n      <td>JP</td>\n      <td>100</td>\n      <td>JP</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>30</td>\n      <td>2020</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Data Engineering Manager</td>\n      <td>51999</td>\n      <td>EUR</td>\n      <td>59303.0</td>\n      <td>DE</td>\n      <td>100</td>\n      <td>DE</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>18</td>\n      <td>2020</td>\n      <td>EN</td>\n      <td>FT</td>\n      <td>Data Science Consultant</td>\n      <td>423000</td>\n      <td>INR</td>\n      <td>5707.0</td>\n      <td>IN</td>\n      <td>50</td>\n      <td>IN</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>117</td>\n      <td>2021</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Data Science Engineer</td>\n      <td>34000</td>\n      <td>EUR</td>\n      <td>40189.0</td>\n      <td>GR</td>\n      <td>100</td>\n      <td>GR</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>67</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Data Science Manager</td>\n      <td>190200</td>\n      <td>USD</td>\n      <td>190200.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0</td>\n      <td>2020</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Data Scientist</td>\n      <td>70000</td>\n      <td>EUR</td>\n      <td>79833.0</td>\n      <td>DE</td>\n      <td>0</td>\n      <td>DE</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>165</td>\n      <td>2021</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Data Specialist</td>\n      <td>165000</td>\n      <td>USD</td>\n      <td>165000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>111</td>\n      <td>2021</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Director of Data Engineering</td>\n      <td>82500</td>\n      <td>GBP</td>\n      <td>113476.0</td>\n      <td>GB</td>\n      <td>100</td>\n      <td>GB</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>2020</td>\n      <td>EX</td>\n      <td>FT</td>\n      <td>Director of Data Science</td>\n      <td>325000</td>\n      <td>USD</td>\n      <td>325000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>373</td>\n      <td>2022</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>ETL Developer</td>\n      <td>50000</td>\n      <td>EUR</td>\n      <td>54957.0</td>\n      <td>GR</td>\n      <td>0</td>\n      <td>GR</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>183</td>\n      <td>2021</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Finance Data Analyst</td>\n      <td>45000</td>\n      <td>GBP</td>\n      <td>61896.0</td>\n      <td>GB</td>\n      <td>50</td>\n      <td>GB</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>97</td>\n      <td>2021</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Financial Data Analyst</td>\n      <td>450000</td>\n      <td>USD</td>\n      <td>450000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>74</td>\n      <td>2021</td>\n      <td>EX</td>\n      <td>FT</td>\n      <td>Head of Data</td>\n      <td>235000</td>\n      <td>USD</td>\n      <td>235000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>161</td>\n      <td>2021</td>\n      <td>EX</td>\n      <td>FT</td>\n      <td>Head of Data Science</td>\n      <td>85000</td>\n      <td>USD</td>\n      <td>85000.0</td>\n      <td>RU</td>\n      <td>0</td>\n      <td>RU</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>384</td>\n      <td>2022</td>\n      <td>EX</td>\n      <td>FT</td>\n      <td>Head of Machine Learning</td>\n      <td>6000000</td>\n      <td>INR</td>\n      <td>79039.0</td>\n      <td>IN</td>\n      <td>50</td>\n      <td>IN</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>13</td>\n      <td>2020</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Lead Data Analyst</td>\n      <td>87000</td>\n      <td>USD</td>\n      <td>87000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>9</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Lead Data Engineer</td>\n      <td>125000</td>\n      <td>USD</td>\n      <td>125000.0</td>\n      <td>NZ</td>\n      <td>50</td>\n      <td>NZ</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>6</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Lead Data Scientist</td>\n      <td>190000</td>\n      <td>USD</td>\n      <td>190000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>457</td>\n      <td>2022</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Lead Machine Learning Engineer</td>\n      <td>80000</td>\n      <td>EUR</td>\n      <td>87932.0</td>\n      <td>DE</td>\n      <td>0</td>\n      <td>DE</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>45</td>\n      <td>2020</td>\n      <td>EN</td>\n      <td>PT</td>\n      <td>ML Engineer</td>\n      <td>14000</td>\n      <td>EUR</td>\n      <td>15966.0</td>\n      <td>DE</td>\n      <td>100</td>\n      <td>DE</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>130</td>\n      <td>2021</td>\n      <td>EN</td>\n      <td>FT</td>\n      <td>Machine Learning Developer</td>\n      <td>100000</td>\n      <td>USD</td>\n      <td>100000.0</td>\n      <td>IQ</td>\n      <td>50</td>\n      <td>IQ</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>4</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Machine Learning Engineer</td>\n      <td>150000</td>\n      <td>USD</td>\n      <td>150000.0</td>\n      <td>US</td>\n      <td>50</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>42</td>\n      <td>2020</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Machine Learning Infrastructure Engineer</td>\n      <td>44000</td>\n      <td>EUR</td>\n      <td>50180.0</td>\n      <td>PT</td>\n      <td>0</td>\n      <td>PT</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>29</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Machine Learning Manager</td>\n      <td>157000</td>\n      <td>CAD</td>\n      <td>117104.0</td>\n      <td>CA</td>\n      <td>50</td>\n      <td>CA</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>1</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Machine Learning Scientist</td>\n      <td>260000</td>\n      <td>USD</td>\n      <td>260000.0</td>\n      <td>JP</td>\n      <td>0</td>\n      <td>JP</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>90</td>\n      <td>2021</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Marketing Data Analyst</td>\n      <td>75000</td>\n      <td>EUR</td>\n      <td>88654.0</td>\n      <td>GR</td>\n      <td>100</td>\n      <td>DK</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>455</td>\n      <td>2022</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>NLP Engineer</td>\n      <td>240000</td>\n      <td>CNY</td>\n      <td>37236.0</td>\n      <td>US</td>\n      <td>50</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>249</td>\n      <td>2021</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Principal Data Analyst</td>\n      <td>170000</td>\n      <td>USD</td>\n      <td>170000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>121</td>\n      <td>2021</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Principal Data Engineer</td>\n      <td>200000</td>\n      <td>USD</td>\n      <td>200000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>55</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Principal Data Scientist</td>\n      <td>130000</td>\n      <td>EUR</td>\n      <td>148261.0</td>\n      <td>DE</td>\n      <td>100</td>\n      <td>DE</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>3</td>\n      <td>2020</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Product Data Analyst</td>\n      <td>20000</td>\n      <td>USD</td>\n      <td>20000.0</td>\n      <td>HN</td>\n      <td>0</td>\n      <td>HN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>26</td>\n      <td>2020</td>\n      <td>EN</td>\n      <td>FT</td>\n      <td>Research Scientist</td>\n      <td>42000</td>\n      <td>USD</td>\n      <td>42000.0</td>\n      <td>NL</td>\n      <td>50</td>\n      <td>NL</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>283</td>\n      <td>2021</td>\n      <td>SE</td>\n      <td>CT</td>\n      <td>Staff Data Scientist</td>\n      <td>105000</td>\n      <td>USD</td>\n      <td>105000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# v2 - return full df and selects only first occurrence of each category\n",
    "dropDisDF = df2.dropDuplicates([\"job_title\",])\n",
    "display(dropDisDF.toPandas())\n",
    "print(dropDisDF.count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "print all rows from df_job_titles without truncating jobs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "asian-edition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|job_title                               |\n",
      "+----------------------------------------+\n",
      "|3D Computer Vision Researcher           |\n",
      "|Lead Data Engineer                      |\n",
      "|Head of Machine Learning                |\n",
      "|Data Specialist                         |\n",
      "|Data Analytics Lead                     |\n",
      "|Machine Learning Scientist              |\n",
      "|Lead Data Analyst                       |\n",
      "|Data Engineering Manager                |\n",
      "|Staff Data Scientist                    |\n",
      "|ETL Developer                           |\n",
      "|Director of Data Engineering            |\n",
      "|Product Data Analyst                    |\n",
      "|Principal Data Scientist                |\n",
      "|AI Scientist                            |\n",
      "|Director of Data Science                |\n",
      "|Machine Learning Engineer               |\n",
      "|Lead Data Scientist                     |\n",
      "|Machine Learning Infrastructure Engineer|\n",
      "|Data Science Engineer                   |\n",
      "|Machine Learning Manager                |\n",
      "+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_job_title.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-pharmacy",
   "metadata": {},
   "source": [
    "create  df_analytic that will consists from max, avg, min USD salaries for all job_titles using groupBy. name of fields is avg_salary, min_salary, max_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "naval-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic = df2\\\n",
    "    .filter(col('salary_currency') == 'USD')\\\n",
    "    .groupby(\"job_title\")\\\n",
    "    .agg(avg(\"salary_in_usd\").alias(\"avg_salary\"),\n",
    "         min(\"salary_in_usd\").alias(\"min_salary\"),\n",
    "         max(\"salary_in_usd\").alias(\"max_salary\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-pledge",
   "metadata": {},
   "source": [
    "print all rows from df_analytic without truncating jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------------------+----------+----------+\n",
      "|job_title                               |avg_salary        |min_salary|max_salary|\n",
      "+----------------------------------------+------------------+----------+----------+\n",
      "|Lead Data Engineer                      |154250.0          |56000.0   |276000.0  |\n",
      "|Data Specialist                         |165000.0          |165000.0  |165000.0  |\n",
      "|Data Analytics Lead                     |405000.0          |405000.0  |405000.0  |\n",
      "|Machine Learning Scientist              |158412.5          |12000.0   |260000.0  |\n",
      "|Lead Data Analyst                       |128500.0          |87000.0   |170000.0  |\n",
      "|Data Engineering Manager                |159000.0          |150000.0  |174000.0  |\n",
      "|Staff Data Scientist                    |105000.0          |105000.0  |105000.0  |\n",
      "|Director of Data Engineering            |200000.0          |200000.0  |200000.0  |\n",
      "|Product Data Analyst                    |20000.0           |20000.0   |20000.0   |\n",
      "|Principal Data Scientist                |255500.0          |151000.0  |416000.0  |\n",
      "|AI Scientist                            |79800.0           |12000.0   |200000.0  |\n",
      "|Director of Data Science                |247666.66666666666|168000.0  |325000.0  |\n",
      "|Machine Learning Engineer               |140488.0          |20000.0   |250000.0  |\n",
      "|Lead Data Scientist                     |152500.0          |115000.0  |190000.0  |\n",
      "|Machine Learning Infrastructure Engineer|195000.0          |195000.0  |195000.0  |\n",
      "|Data Science Engineer                   |60000.0           |60000.0   |60000.0   |\n",
      "|Research Scientist                      |139428.42857142858|42000.0   |450000.0  |\n",
      "|Head of Data                            |221666.66666666666|200000.0  |235000.0  |\n",
      "|Cloud Data Engineer                     |160000.0          |160000.0  |160000.0  |\n",
      "|Machine Learning Developer              |100000.0          |100000.0  |100000.0  |\n",
      "+----------------------------------------+------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_analytic.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-color",
   "metadata": {},
   "source": [
    "now you need to add in df_analytic column row_id, that will show order of all job_titles depending on avg salary. they should be descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "window = Window.partitionBy().orderBy(desc(\"avg_salary\"))\n",
    "df_analytic_avg_salary_rowid = df_analytic.withColumn(\"row_id\", row_number().over(window))\n",
    "\n",
    "# used empty Window.partitionBy() instead of partitionBy(\"job_title\") because df_analytics has already grouped by job_title"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "documented-catalog",
   "metadata": {},
   "source": [
    "print all data from df_analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "# to avoid warnings below when show - use following code (add tmp column filled with 1):\n",
    "# df_analytic_partition = df_analytic.withColumn(\"partition\", lit(1))\n",
    "# window = Window.partitionBy(\"partition\").orderBy(desc(\"avg_salary\"))\n",
    "# df_analytic_avg_salary_rowid = df_analytic_partition.withColumn(\"row_id\", row_number().over(window))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "confirmed-monitoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 20:46:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/06 20:46:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/06 20:46:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/06 20:46:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/06 20:46:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/06 20:46:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------------------+----------+----------+------+\n",
      "|job_title                               |avg_salary        |min_salary|max_salary|row_id|\n",
      "+----------------------------------------+------------------+----------+----------+------+\n",
      "|Data Analytics Lead                     |405000.0          |405000.0  |405000.0  |1     |\n",
      "|Principal Data Engineer                 |328333.3333333333 |185000.0  |600000.0  |2     |\n",
      "|Financial Data Analyst                  |275000.0          |100000.0  |450000.0  |3     |\n",
      "|ML Engineer                             |263000.0          |256000.0  |270000.0  |4     |\n",
      "|Principal Data Scientist                |255500.0          |151000.0  |416000.0  |5     |\n",
      "|Director of Data Science                |247666.66666666666|168000.0  |325000.0  |6     |\n",
      "|Applied Data Scientist                  |238000.0          |157000.0  |380000.0  |7     |\n",
      "|Head of Data                            |221666.66666666666|200000.0  |235000.0  |8     |\n",
      "|Director of Data Engineering            |200000.0          |200000.0  |200000.0  |9     |\n",
      "|Machine Learning Infrastructure Engineer|195000.0          |195000.0  |195000.0  |10    |\n",
      "|Applied Machine Learning Scientist      |178800.0          |38400.0   |423000.0  |11    |\n",
      "|Data Architect                          |177873.9090909091 |90700.0   |266400.0  |12    |\n",
      "|Data Science Manager                    |175118.3          |137141.0  |241000.0  |13    |\n",
      "|Analytics Engineer                      |175000.0          |135000.0  |205300.0  |14    |\n",
      "|Data Specialist                         |165000.0          |165000.0  |165000.0  |15    |\n",
      "|Cloud Data Engineer                     |160000.0          |160000.0  |160000.0  |16    |\n",
      "|Data Engineering Manager                |159000.0          |150000.0  |174000.0  |17    |\n",
      "|Machine Learning Scientist              |158412.5          |12000.0   |260000.0  |18    |\n",
      "|Lead Data Engineer                      |154250.0          |56000.0   |276000.0  |19    |\n",
      "|Lead Data Scientist                     |152500.0          |115000.0  |190000.0  |20    |\n",
      "+----------------------------------------+------------------+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_analytic_avg_salary_rowid.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------------------+----------+----------+------+\n",
      "|job_title                               |avg_salary        |min_salary|max_salary|row_id|\n",
      "+----------------------------------------+------------------+----------+----------+------+\n",
      "|Data Analytics Lead                     |405000.0          |405000.0  |405000.0  |1     |\n",
      "|Principal Data Engineer                 |328333.3333333333 |185000.0  |600000.0  |2     |\n",
      "|Financial Data Analyst                  |275000.0          |100000.0  |450000.0  |3     |\n",
      "|ML Engineer                             |263000.0          |256000.0  |270000.0  |4     |\n",
      "|Principal Data Scientist                |255500.0          |151000.0  |416000.0  |5     |\n",
      "|Director of Data Science                |247666.66666666666|168000.0  |325000.0  |6     |\n",
      "|Applied Data Scientist                  |238000.0          |157000.0  |380000.0  |7     |\n",
      "|Head of Data                            |221666.66666666666|200000.0  |235000.0  |8     |\n",
      "|Director of Data Engineering            |200000.0          |200000.0  |200000.0  |9     |\n",
      "|Machine Learning Infrastructure Engineer|195000.0          |195000.0  |195000.0  |10    |\n",
      "|Applied Machine Learning Scientist      |178800.0          |38400.0   |423000.0  |11    |\n",
      "|Data Architect                          |177873.9090909091 |90700.0   |266400.0  |12    |\n",
      "|Data Science Manager                    |175118.3          |137141.0  |241000.0  |13    |\n",
      "|Analytics Engineer                      |175000.0          |135000.0  |205300.0  |14    |\n",
      "|Data Specialist                         |165000.0          |165000.0  |165000.0  |15    |\n",
      "|Cloud Data Engineer                     |160000.0          |160000.0  |160000.0  |16    |\n",
      "|Data Engineering Manager                |159000.0          |150000.0  |174000.0  |17    |\n",
      "|Machine Learning Scientist              |158412.5          |12000.0   |260000.0  |18    |\n",
      "|Lead Data Engineer                      |154250.0          |56000.0   |276000.0  |19    |\n",
      "|Lead Data Scientist                     |152500.0          |115000.0  |190000.0  |20    |\n",
      "+----------------------------------------+------------------+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# v2\n",
    "df_analytic_avg_salary = df_analytic.orderBy(desc(\"avg_salary\"))\n",
    "df_analytic_avg_salary_rowid_2 = df_analytic_avg_salary.withColumn(\"row_id\", monotonically_increasing_id()+1)\n",
    "df_analytic_avg_salary_rowid_2.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "satellite-quarter",
   "metadata": {},
   "source": [
    "it isn\"t beautifull, so we need to put now row_id on first place in df_analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ranging-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic_avg_salary_rowid = df_analytic_avg_salary_rowid.select(\"row_id\", \"job_title\", \"avg_salary\", \"min_salary\", \"max_salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-amsterdam",
   "metadata": {},
   "source": [
    "print df_analytic now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "classical-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------+------------------+----------+----------+\n",
      "|row_id|job_title                               |avg_salary        |min_salary|max_salary|\n",
      "+------+----------------------------------------+------------------+----------+----------+\n",
      "|1     |Data Analytics Lead                     |405000.0          |405000.0  |405000.0  |\n",
      "|2     |Principal Data Engineer                 |328333.3333333333 |185000.0  |600000.0  |\n",
      "|3     |Financial Data Analyst                  |275000.0          |100000.0  |450000.0  |\n",
      "|4     |ML Engineer                             |263000.0          |256000.0  |270000.0  |\n",
      "|5     |Principal Data Scientist                |255500.0          |151000.0  |416000.0  |\n",
      "|6     |Director of Data Science                |247666.66666666666|168000.0  |325000.0  |\n",
      "|7     |Applied Data Scientist                  |238000.0          |157000.0  |380000.0  |\n",
      "|8     |Head of Data                            |221666.66666666666|200000.0  |235000.0  |\n",
      "|9     |Director of Data Engineering            |200000.0          |200000.0  |200000.0  |\n",
      "|10    |Machine Learning Infrastructure Engineer|195000.0          |195000.0  |195000.0  |\n",
      "|11    |Applied Machine Learning Scientist      |178800.0          |38400.0   |423000.0  |\n",
      "|12    |Data Architect                          |177873.9090909091 |90700.0   |266400.0  |\n",
      "|13    |Data Science Manager                    |175118.3          |137141.0  |241000.0  |\n",
      "|14    |Analytics Engineer                      |175000.0          |135000.0  |205300.0  |\n",
      "|15    |Data Specialist                         |165000.0          |165000.0  |165000.0  |\n",
      "|16    |Cloud Data Engineer                     |160000.0          |160000.0  |160000.0  |\n",
      "|17    |Data Engineering Manager                |159000.0          |150000.0  |174000.0  |\n",
      "|18    |Machine Learning Scientist              |158412.5          |12000.0   |260000.0  |\n",
      "|19    |Lead Data Engineer                      |154250.0          |56000.0   |276000.0  |\n",
      "|20    |Lead Data Scientist                     |152500.0          |115000.0  |190000.0  |\n",
      "+------+----------------------------------------+------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 20:46:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/06 20:46:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/06 20:46:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/06 20:46:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/06 20:46:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/06 20:46:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df_analytic_avg_salary_rowid.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-queensland",
   "metadata": {},
   "source": [
    "here you need to create df_exp_lvl with the biggest usd_salary(biggest_salary) for each experience_level(you need to save all fields like in entire dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "  experience_level  biggest_salary   id  work_year employment_type  \\\n0               EX        600000.0  252       2021              FT   \n1               MI        450000.0   97       2021              FT   \n2               MI        450000.0   33       2020              FT   \n3               MI        450000.0   21       2020              FT   \n4               EN        250000.0   37       2020              FT   \n5               SE        412000.0   63       2020              FT   \n\n                   job_title salary_currency  salary_in_usd  \\\n0    Principal Data Engineer             USD       600000.0   \n1     Financial Data Analyst             USD       450000.0   \n2         Research Scientist             USD       450000.0   \n3       Product Data Analyst             INR         6072.0   \n4  Machine Learning Engineer             USD       250000.0   \n5             Data Scientist             USD       412000.0   \n\n  employee_residence  remote_ratio company_location company_size  \n0                 US           100               US            L  \n1                 US           100               US            L  \n2                 US             0               US            M  \n3                 IN           100               IN            L  \n4                 US            50               US            L  \n5                 US           100               US            L  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>experience_level</th>\n      <th>biggest_salary</th>\n      <th>id</th>\n      <th>work_year</th>\n      <th>employment_type</th>\n      <th>job_title</th>\n      <th>salary_currency</th>\n      <th>salary_in_usd</th>\n      <th>employee_residence</th>\n      <th>remote_ratio</th>\n      <th>company_location</th>\n      <th>company_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EX</td>\n      <td>600000.0</td>\n      <td>252</td>\n      <td>2021</td>\n      <td>FT</td>\n      <td>Principal Data Engineer</td>\n      <td>USD</td>\n      <td>600000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>97</td>\n      <td>2021</td>\n      <td>FT</td>\n      <td>Financial Data Analyst</td>\n      <td>USD</td>\n      <td>450000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>33</td>\n      <td>2020</td>\n      <td>FT</td>\n      <td>Research Scientist</td>\n      <td>USD</td>\n      <td>450000.0</td>\n      <td>US</td>\n      <td>0</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>21</td>\n      <td>2020</td>\n      <td>FT</td>\n      <td>Product Data Analyst</td>\n      <td>INR</td>\n      <td>6072.0</td>\n      <td>IN</td>\n      <td>100</td>\n      <td>IN</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>EN</td>\n      <td>250000.0</td>\n      <td>37</td>\n      <td>2020</td>\n      <td>FT</td>\n      <td>Machine Learning Engineer</td>\n      <td>USD</td>\n      <td>250000.0</td>\n      <td>US</td>\n      <td>50</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>SE</td>\n      <td>412000.0</td>\n      <td>63</td>\n      <td>2020</td>\n      <td>FT</td>\n      <td>Data Scientist</td>\n      <td>USD</td>\n      <td>412000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# v1 - regardless of salary_currency: (ex. USD, INR)\n",
    "df_exp_lvl_group = df2.groupby(\"experience_level\").agg(max(\"salary_in_usd\").alias(\"salary\"))\n",
    "df_exp_lvl = df_exp_lvl_group.join(df2, [\"experience_level\", \"salary\"], 'left')\n",
    "df_exp_lvl = df_exp_lvl.withColumnRenamed(\"salary\", \"biggest_salary\")\n",
    "display(df_exp_lvl.toPandas())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "    id  work_year experience_level employment_type                  job_title  \\\n0   37       2020               EN              FT  Machine Learning Engineer   \n1  252       2021               EX              FT    Principal Data Engineer   \n2   33       2020               MI              FT         Research Scientist   \n3   97       2021               MI              FT     Financial Data Analyst   \n4   63       2020               SE              FT             Data Scientist   \n\n   biggest_salary salary_currency  salary_in_usd employee_residence  \\\n0          250000             USD       250000.0                 US   \n1          600000             USD       600000.0                 US   \n2          450000             USD       450000.0                 US   \n3          450000             USD       450000.0                 US   \n4          412000             USD       412000.0                 US   \n\n   remote_ratio company_location company_size  salary_rank_per_level  \n0            50               US            L                      1  \n1           100               US            L                      1  \n2             0               US            M                      1  \n3           100               US            L                      1  \n4           100               US            L                      1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>work_year</th>\n      <th>experience_level</th>\n      <th>employment_type</th>\n      <th>job_title</th>\n      <th>biggest_salary</th>\n      <th>salary_currency</th>\n      <th>salary_in_usd</th>\n      <th>employee_residence</th>\n      <th>remote_ratio</th>\n      <th>company_location</th>\n      <th>company_size</th>\n      <th>salary_rank_per_level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37</td>\n      <td>2020</td>\n      <td>EN</td>\n      <td>FT</td>\n      <td>Machine Learning Engineer</td>\n      <td>250000</td>\n      <td>USD</td>\n      <td>250000.0</td>\n      <td>US</td>\n      <td>50</td>\n      <td>US</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>252</td>\n      <td>2021</td>\n      <td>EX</td>\n      <td>FT</td>\n      <td>Principal Data Engineer</td>\n      <td>600000</td>\n      <td>USD</td>\n      <td>600000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33</td>\n      <td>2020</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Research Scientist</td>\n      <td>450000</td>\n      <td>USD</td>\n      <td>450000.0</td>\n      <td>US</td>\n      <td>0</td>\n      <td>US</td>\n      <td>M</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>97</td>\n      <td>2021</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Financial Data Analyst</td>\n      <td>450000</td>\n      <td>USD</td>\n      <td>450000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>63</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Data Scientist</td>\n      <td>412000</td>\n      <td>USD</td>\n      <td>412000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# v2 - only USD currency\n",
    "windowExp = Window.partitionBy(\"experience_level\").orderBy(col('salary_in_usd').desc())\n",
    "df_exp_level2 = df2.withColumn(\"salary_rank_per_level\", rank().over(windowExp)).filter(col(\"salary_rank_per_level\") == 1)\n",
    "df_exp_level2 = df_exp_level2.withColumnRenamed(\"salary\", \"biggest_salary\")\n",
    "display(df_exp_level2.toPandas())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-mortgage",
   "metadata": {},
   "source": [
    "create df_best that consists from rows where salary of guy same as biggest salary for other people in his exp_lvl and choose only columns: id, experience_level, biggest_salary, employee_residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "toxic-prompt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "     id experience_level  biggest_salary employee_residence\n0   252               EX        600000.0                 US\n1    97               MI        450000.0                 US\n2    97               MI        450000.0                 US\n3    97               MI        450000.0                 US\n4    33               MI        450000.0                 US\n5    33               MI        450000.0                 US\n6    33               MI        450000.0                 US\n7    21               MI        450000.0                 IN\n8    21               MI        450000.0                 IN\n9    21               MI        450000.0                 IN\n10   37               EN        250000.0                 US\n11   63               SE        412000.0                 US",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>experience_level</th>\n      <th>biggest_salary</th>\n      <th>employee_residence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>252</td>\n      <td>EX</td>\n      <td>600000.0</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>97</td>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>97</td>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>97</td>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33</td>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>33</td>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>33</td>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>21</td>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>21</td>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>21</td>\n      <td>MI</td>\n      <td>450000.0</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>37</td>\n      <td>EN</td>\n      <td>250000.0</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>63</td>\n      <td>SE</td>\n      <td>412000.0</td>\n      <td>US</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_best = df_exp_lvl.join(df2, \"experience_level\", 'inner').where(expr(\"salary = biggest_salary\"))\n",
    "df_best = df_best.select(df_exp_lvl.id, df_exp_lvl.experience_level, df_exp_lvl.biggest_salary, df_exp_lvl.employee_residence)\n",
    "display(df_best.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "    id experience_level  salary employee_residence\n0   37               EN  250000                 US\n1  252               EX  600000                 US\n2   33               MI  450000                 US\n3   97               MI  450000                 US\n4   63               SE  412000                 US",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>experience_level</th>\n      <th>salary</th>\n      <th>employee_residence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37</td>\n      <td>EN</td>\n      <td>250000</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>252</td>\n      <td>EX</td>\n      <td>600000</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33</td>\n      <td>MI</td>\n      <td>450000</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>97</td>\n      <td>MI</td>\n      <td>450000</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>63</td>\n      <td>SE</td>\n      <td>412000</td>\n      <td>US</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# v2\n",
    "windowBest = Window.partitionBy('experience_level').orderBy(col('salary_in_USD').desc())\n",
    "df_best2 = df2.withColumn('salary_rank_per_level', rank().over(windowBest)).filter(col('salary_rank_per_level') == 1)\n",
    "df_best2 = df_best2.select(\"id\", \"experience_level\", \"salary\", \"employee_residence\")\n",
    "display(df_best2.toPandas())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "integral-brass",
   "metadata": {},
   "source": [
    "drop duplicates if exist by experience_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "immune-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------------+------------------+\n",
      "| id|experience_level|biggest_salary|employee_residence|\n",
      "+---+----------------+--------------+------------------+\n",
      "| 37|              EN|      250000.0|                US|\n",
      "|252|              EX|      600000.0|                US|\n",
      "| 97|              MI|      450000.0|                US|\n",
      "| 63|              SE|      412000.0|                US|\n",
      "+---+----------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_best_drop_duplicates = df_best.dropDuplicates([\"experience_level\"])\n",
    "df_best_drop_duplicates.show()\n",
    "# dropped id=33 - but values the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "specified-wellington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------+------------------+\n",
      "| id|experience_level|salary|employee_residence|\n",
      "+---+----------------+------+------------------+\n",
      "| 37|              EN|250000|                US|\n",
      "|252|              EX|600000|                US|\n",
      "| 33|              MI|450000|                US|\n",
      "| 63|              SE|412000|                US|\n",
      "+---+----------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_best_drop_duplicates = df_best2.dropDuplicates([\"experience_level\"])\n",
    "df_best_drop_duplicates.show()\n",
    "# dropped id=97 - but values the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-plant",
   "metadata": {},
   "source": [
    "create df_new_best from df_best without id, and make the next: when exp_level = exp_level we want middle, when SE we want senior, else Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "infinite-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_best = df_best_drop_duplicates.drop(\"id\")\n",
    "# v2\n",
    "# df_new_best = df_best.select([col for col in df_best.columns if col != \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "df_new_best_ = df_new_best.withColumn(\"new_experience_level\",    # to replace current col use \"experience_level\"\n",
    "                                      when(df_new_best.experience_level == \"MI\", \"middle\")\n",
    "                                     .when(df_new_best.experience_level == \"SE\", \"senior\")\n",
    "                                     .otherwise(None))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "changing-fairy",
   "metadata": {},
   "source": [
    "print df_new_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "endless-framework",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+------------------+--------------------+\n",
      "|experience_level|salary|employee_residence|new_experience_level|\n",
      "+----------------+------+------------------+--------------------+\n",
      "|              EN|250000|                US|                NULL|\n",
      "|              EX|600000|                US|                NULL|\n",
      "|              MI|450000|                US|              middle|\n",
      "|              SE|412000|                US|              senior|\n",
      "+----------------+------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_best_.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-status",
   "metadata": {},
   "source": [
    "write df_new_best like 1.csv and load then it to df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baking-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_best_.write.csv(\"1.csv\", header=True, mode='overwrite')  # folder is created\n",
    "df_new_best_.toPandas().to_csv(\"2.csv\", index=False)     # single file is created\n",
    "df_final = spark.read.csv(\"1.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-shooting",
   "metadata": {},
   "source": [
    "print df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "expired-viewer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+------------------+--------------------+\n",
      "|experience_level|salary|employee_residence|new_experience_level|\n",
      "+----------------+------+------------------+--------------------+\n",
      "|              EN|250000|                US|                NULL|\n",
      "|              EX|600000|                US|                NULL|\n",
      "|              MI|450000|                US|              middle|\n",
      "|              SE|412000|                US|              senior|\n",
      "+----------------+------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-progress",
   "metadata": {},
   "source": [
    "filter df_final to delete experience_level where it Null, then join this table by biggest_salary(salary_in_usd) and employee_residence with entire df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "small-polymer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 21:53:27 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  experience_level  salary employee_residence new_experience_level  id  \\\n0               MI  450000                 US               middle  33   \n1               SE  412000                 US               senior  63   \n\n   work_year experience_level employment_type           job_title  salary  \\\n0       2020               MI              FT  Research Scientist  450000   \n1       2020               SE              FT      Data Scientist  412000   \n\n  salary_currency  salary_in_usd employee_residence  remote_ratio  \\\n0             USD       450000.0                 US             0   \n1             USD       412000.0                 US           100   \n\n  company_location company_size  \n0               US            M  \n1               US            L  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>experience_level</th>\n      <th>salary</th>\n      <th>employee_residence</th>\n      <th>new_experience_level</th>\n      <th>id</th>\n      <th>work_year</th>\n      <th>experience_level</th>\n      <th>employment_type</th>\n      <th>job_title</th>\n      <th>salary</th>\n      <th>salary_currency</th>\n      <th>salary_in_usd</th>\n      <th>employee_residence</th>\n      <th>remote_ratio</th>\n      <th>company_location</th>\n      <th>company_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MI</td>\n      <td>450000</td>\n      <td>US</td>\n      <td>middle</td>\n      <td>33</td>\n      <td>2020</td>\n      <td>MI</td>\n      <td>FT</td>\n      <td>Research Scientist</td>\n      <td>450000</td>\n      <td>USD</td>\n      <td>450000.0</td>\n      <td>US</td>\n      <td>0</td>\n      <td>US</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SE</td>\n      <td>412000</td>\n      <td>US</td>\n      <td>senior</td>\n      <td>63</td>\n      <td>2020</td>\n      <td>SE</td>\n      <td>FT</td>\n      <td>Data Scientist</td>\n      <td>412000</td>\n      <td>USD</td>\n      <td>412000.0</td>\n      <td>US</td>\n      <td>100</td>\n      <td>US</td>\n      <td>L</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_final_filtered = df_final.filter(df_final.new_experience_level.isNotNull())\n",
    "# df_final_filtered = df_final.na.drop()\n",
    "df_joined_final = df_final_filtered.join(df2,\n",
    "            (df2.salary_in_usd == df_final_filtered.salary) & (df2.employee_residence == df_final_filtered.employee_residence),\n",
    "            \"inner\").dropDuplicates([\"experience_level\"])\n",
    "display(df_joined_final.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-moore",
   "metadata": {},
   "source": [
    "last task is to save in variable and then print this variable of the biggest salary_in_usd from df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "individual-institution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest salary_in_usd from df_final is: 450000.0\n"
     ]
    }
   ],
   "source": [
    "max_salary = df_joined_final.select(col('salary_in_usd')).collect()[0][0]\n",
    "print(\"The biggest salary_in_usd from df_final is:\", max_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-procedure",
   "metadata": {},
   "source": [
    "It is the end of PySpark basics. In other lessons you will learn optimizations technics and how to make distributed system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
