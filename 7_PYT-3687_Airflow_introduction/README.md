Task 7. Airflow introduction
Description

В данном задании необходимо локально развернуть и настроить рабочее окружение со всеми необходимыми инструментами:

    Airflow;

    Python;

    Pandas;

    MongoDB.

В данном задании необходимо создать DAG для обработки данных (Airflow Data) и загрузке их в MongoDB.

DAG должен состоять из нескольких задач:

    Создайте Sensor который будет реагировать на появление нашего файла с данными в папке.

    После срабатывания Сенсора должен запуститься task.branch, который проверяет пустой файл или нет.

     Основной флоу

        Если файл пустой, bash скрипт, который логирует факт, что файл пустой

        Если файл содержит данные, должны запуститься следующие задачи, которые будут отвечать за обработку данных. Все задачи по обработке данных должны быть объединены в отдельный TaskGroup. Каждая задача должна отвечать за отдельный функционал:

    Заменить все "null" значения на "-";

    Отсортировать данные по created_date;

    удалите все ненужные символы из content колонки (например, смайлики и т.д.), оставьте только текст и знаки препинания.

    После выполнения всех трансформационнных задач, используя

Dataset Data-aware scheduling — Airflow Documentation (apache.org) создайте второй DAG, который будет тригериться изменением финального датасета и будет загружать обработанные данные в MongoDB. Для этого также необходимо локально настроить MongoDB и конфигурацию Connections в Airflow.

После того, как вы перенесли все обработанные данные в MongoDB, выполните следующие запросы (непосредственно в MongoDB, например, вкладка Aggregations в MongoDB Compass):

    Топ-5 часто встречаемых комментариев

    Все записи, где длина поля “content” составляет менее 5 символов;

    Средний рейтинг по каждому дню (результат должен быть в виде timestamp type).